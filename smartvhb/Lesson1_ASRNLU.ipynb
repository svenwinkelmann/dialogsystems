{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ruse-Based Dialogsystems: ASR and NLU\n",
    "Goals of today's exercise:\n",
    "- Transcribe audio using the Google ASR cloud service\n",
    "- Transcribe audio using vosk as a local speech recognition engine \n",
    "- NLU language understanding with Stanford stanza engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will get to know a library that connects various speech recognizers and online APIs. We use the Google Speech API as a cloud service and Vosk as a speech recognizer, which works locally.\n",
    "\n",
    "Install the following library: https://pypi.org/project/SpeechRecognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "print(f\"We use speech_recognition version: {sr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To record the microphone input and forward it to the speech recognizer, we use the PyAudio library. Install PyAudio with pip. Depending on the operating system, it may be necessary to install additional packages with homebrew (macOS) or apt (ubuntu). You will surely find a solution with Google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaudio import PyAudio\n",
    "\n",
    "p = PyAudio()\n",
    "try:\n",
    "    print(p.get_default_input_device_info())\n",
    "except:\n",
    "    print(\"No mics available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe a spoken utterance\n",
    "\n",
    "The following tutorial provides good instructions on how to use the SpeechRecognition library: https://www.simplilearn.com/tutorials/python-tutorial/speech-recognition-in-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "recognizer = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture a spoken utterance with the microphone. Say for example: \"Call Paul on his mobile number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sr.Microphone() as source:\n",
    "    print(\"Speak something...\")\n",
    "    audio_data = recognizer.listen(source)\n",
    "    print(\"Audio data recorded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we send the captured utterance to the Google Web Speech API. If the API is not accessible, the ```recognizer``` throws a ```RequestError```. If the passed ```audio_data``` does not contain a speech utterance, the ```recognizer``` throws an ```UnknownValueError```. We check for both exceptions and give the user appropriate feedback on the console.\n",
    "\n",
    "We would like to look at several recognition results and the respective confidence. With the ```show_all``` parameter, the ```recognizer``` outputs several alternatives as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def recognize_google(audio_data):\n",
    "    try:\n",
    "        result = recognizer.recognize_google(audio_data, language='en-US', show_all=True)\n",
    "        print(f\"[Google] Recognition results: {json.dumps(result, sort_keys=True, indent=4)}\")\n",
    "        return result\n",
    "    except sr.RequestError:\n",
    "        print(\"[Google] RequestError: Could not access Google Web Speech API\")\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"[Google] UnknownValueError: Sorry, I do not understand\")\n",
    "\n",
    "google_result = recognize_google(audio_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "We define a function ```recognize_vosk(audio_data, ..)``` for recognition and return the recognition result. Good programming practice is to pass all necessary objects as parameters rather than relying on global variables from the Jupyter Notebook. This allows the function to be easily copied into other notebooks without introducing global dependencies. The vosk recognizer runs locally without sending data to the cloud. Therefore, we need to download a model from https://alphacephei.com/vosk/models first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "if not os.path.exists('model'):\n",
    "    model_name = 'vosk-model-small-en-us-0.15.zip'\n",
    "    model_url = 'https://alphacephei.com/vosk/models/' + model_name\n",
    "    urllib.request.urlretrieve(model_url, model_name)\n",
    "\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(model_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "\n",
    "    import os\n",
    "    os.rename(model_name.replace('.zip', ''), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def recognize_vosk(audio_data: any, recognizer: sr.Recognizer, language = 'en') -> any:\n",
    "    try:\n",
    "        text = recognizer.recognize_vosk(audio_data, language='de')\n",
    "        return json.loads(text)\n",
    "    except sr.RequestError:\n",
    "        print(\"[Vosk] Error: Could not access Google Web Speech API;\")\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"[Vosk] Sorry, I do not understand\")\n",
    "    return None\n",
    "\n",
    "vosk_result = recognize_vosk(audio_data, recognizer)\n",
    "print(f\"[Vosk] Recognition Result: {vosk_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use the Stanford CoreNLP to analyze the results from Google and Vosk. There is a Python implementation: https://stanfordnlp.github.io/stanza/ . Install it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the stanza pipeline tokenize, pos, lemma, depparse, and ner to interpret the results. For more information about the stanza pipeline see: https://stanfordnlp.github.io/stanza/neural_pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, pos, lemma, depparse, ner')\n",
    "\n",
    "print(\"Google NLU result: \", nlp(google_result['alternative'][0]['transcript']))\n",
    "\n",
    "print(\"Vosk NLU result: \", nlp(vosk_result['text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
