{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "Heute möchten wir ein LSTM trainieren, um Äußerungen zu klassifizieren. Wir können dies für unterschiedlichste Anwendungsfälle nutzen, immer wenn ein Text zu einer Klasse zugeordnet werden muss. Dies kann ein Intent sein, aber auch eine Emotion oder Sentiment (positive, negative Stimmung).\n",
    "\n",
    "Um ein LSTM zu trainieren setzen wir die Bibliotheken ```keras``` und ```tensorflow``` ein, installiere diese:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install tensorflow-macos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install tensorflow-metal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir erstellen eine Liste an Sätzen und dazugehörige Labels. Such dir gerne einen Datensatz, der dich interessiert. https://huggingface.co/ oder https://www.kaggle.com/ sind gute Anlaufstellen, dort Datasets und mit ```sentiment``` oder  ```ìntent``` suchen. Zur Einbindung des Datensatzes kannst du ihn entweder herunterladen oder direkt über ```huggingface_hub``` (Anmeldung notwendig) einbinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "# the dataset is split into test and train, however, we want to make our own splits later on, thus we merge them into one single dataframe\n",
    "df = pd.concat([pd.DataFrame(ds['test']), pd.DataFrame(ds['train'])])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei realen Datensätzen ist es meist hilfreich explorativ den Datensatz zu untersuchen. Du kannst die nltk Bibliothek nutzen, um die Anzahl der Sätze und Wörter herausbekomen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "longest_text = max(df['text'], key=len)\n",
    "vocabulary = []\n",
    "for t in df['text']:\n",
    "    vocabulary += word_tokenize(t, language='english')\n",
    "vocabulary = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classes: \", set(df['label_text']))\n",
    "print(\"Longest text: \", len(longest_text))\n",
    "print(\"Vocabulary size: \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein guter Programmierstil ist die Hyperparameter als Konstanten anzulegen, um damit später etwas zu spielen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 100\n",
    "LSTM_UNITS = 64\n",
    "NUM_CLASSES = len(set(df['label_text']))\n",
    "# eigentlich sollte man die längeste Textsequenz nehmen. Bei Texten ungleicher Länge führt dies jedoch zu \n",
    "# sparse-Vektoren, was das Training erschwert.\n",
    "TOKENIZATION_OUTPUT_SEQUENCE_LENGTH = 33\n",
    "TEST_SIZE = 0.1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 35\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir müssen unsere Texte in Zahlenvektoren überführen. Hierfür ist ```TextVectorization``` hilfreich, das aus einem Text ein Vokabular aufbaut (```adapt()```) und anschließend den Text vektorisiert. Wie üblich bei Klassifikationsproblemen kodieren wir die Labels als Hot-Encoded Vektoren (```LabelEncoder```). Anschlißend nutzen wir ```train_test_split()```, um unseren Datensatz in einen Test- und Trainingsdaten zu splitten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "vectorize_layer = TextVectorization(output_sequence_length=TOKENIZATION_OUTPUT_SEQUENCE_LENGTH)\n",
    "vectorize_layer.adapt(df['text'])\n",
    "text_encoded = vectorize_layer(df['text']).numpy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['label_text'])\n",
    "labels_categorical = tf.keras.utils.to_categorical(labels, NUM_CLASSES)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_encoded, labels_categorical, test_size=TEST_SIZE, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kompilieren wir unser LSTM Modell mit ```keras``` und trainieren das Netz, hilfreiche Layer sind ```Èmbedding```, ```LSTM```, ```Dense```, die sequentiell hintereinander ausgeführt werden. Wir trainieren unsere Embeddings hier direkt mit in den Modell-Layern:\n",
    "- Embedding: https://keras.io/api/layers/core_layers/embedding/\n",
    "- LSTM: https://keras.io/api/layers/recurrent_layers/lstm/ \n",
    "- Dense: https://keras.io/api/layers/core_layers/dense/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary), EMBEDDING_DIMENSION, input_length = X_train.shape[1]))\n",
    "model.add(LSTM(LSTM_UNITS))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie sehen die Loss-Curves aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, EPOCHS + 1), history.history['loss'], label='Training Loss', marker='o')\n",
    "plt.plot(range(1, EPOCHS + 1), history.history['val_loss'], label='Validation Loss', marker='x')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach dem Training können wir einzelne Sätze ausprobieren. Gebe dem Modell einen Text und lasse ihn klassifizieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "twt_s = \"I love it\"\n",
    "twt = vectorize_layer(twt_s).numpy()\n",
    "result = model.predict(twt.reshape(1, X_train.shape[1]), batch_size=BATCH_SIZE, verbose=\"auto\")\n",
    "\n",
    "class_names = ['negative', 'neutral', 'positive']\n",
    "predicted_class_number = np.argmax(result)\n",
    "\n",
    "\n",
    "print(f\"'{twt_s}' is {class_names[predicted_class_number]} ({predicted_class_number})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wird es spannend :). Wie schneidet unser Modell mit dem Testdatensatz ab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,acc = model.evaluate(X_test, y_test, verbose = 2, batch_size = BATCH_SIZE)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
