{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition\n",
    "\n",
    "Ziele der heutigen Übung:\n",
    "- Kennenlernen der SpeechRecognition Bibliothek\n",
    "- Transkribieren einer Spracheingabe mit der Google Web Speech API\n",
    "- Verstehen von Erkennerergebnissen eines Spracherkenners\n",
    "- Erleben von Herausforderungen der Spracherkennung\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition API\n",
    "\n",
    "Heute lernen wir eine Bibliothek kennen, welche verschiedene Spracherkenner und online APIs anbindet. Wir verwenden hierfür die Google Speech API als Cloud Service und Vosk als Spracherkenner, welcher lokal mit begrenzten Ressourcen arbeitet.\n",
    "\n",
    "Installiere folgende Bibliothek: https://pypi.org/project/SpeechRecognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "print(f\"We use speech_recognition version: {sr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Eingabe des Mikrofons aufzunehmen und an den Spracherkenner weiter zu leiten, nutzen wir die Bibliothek PyAudio. Installiere PyAudio mit pip. Je nach Betriebssystem kann es erforderlich sein zusätzlieh Pakete mit homebrew (macOS) oder apt (ubuntu) zu installieren. Du findest sicherlich mit Google eine Lösung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaudio import PyAudio\n",
    "\n",
    "p = PyAudio()\n",
    "try:\n",
    "    print(p.get_default_input_device_info())\n",
    "except:\n",
    "    print(\"No mics available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transkribieren einer sprachlichen Äußerung mit der Google Web Speech API (cloud-basiert)\n",
    "\n",
    "Folgendes Tutorial gibt gute Hinweise, wie wir die SpeechRecognition Bibliothek nutzen: https://realpython.com/python-speech-recognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erfasse eine sprachliche Äußerung mit dem Mikrofon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sr.Microphone() as source:\n",
    "    print(\"Speak something...\")\n",
    "    audio_data = recognizer.listen(source)\n",
    "    print(\"Audio data recorded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun schicken wir die erfasste Äußerung an die Google Web Speech API. Sofern die API nicht erreichbar ist, wirft der ```recognizer``` einen ```RequestError```. Sofern in dem übergebenen ```audio_data``` keine sprachliche Äußerung enthalten ist gibt der ```recognizer``` einen ```UnknownValueError``` aus. Prüfe auf beide Exceptions und geben dem Nutzer ein entsprechendes Feedback auf der Konsole aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    text = recognizer.recognize_google(audio_data)\n",
    "    print(\"You said:\", text)\n",
    "except sr.RequestError:\n",
    "    print(\"Error: Could not access Google Web Speech API;\")\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Sorry, I do not understand\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neben Englisch kann die Google Web Speech API auch Deutsch. Passe den Code entsprechend an, dass Deutsch erkannt wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    text = recognizer.recognize_google(audio_data, language='de-DE')\n",
    "    print(\"Du hast gesagt: \", text)\n",
    "except sr.RequestError:\n",
    "    print(\"Fehler: Google Web Speech API ist nicht erreichbar\")\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Sorry, Ich verstehe die Äußerung nicht\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden in der nächsten Vorlesung genauer auf die Technik der Spracherkennung schauen. Heute möchten wir uns jedoch bereits mehrer Erkennerergebnisse und die jeweilige Konfidenz anschauen. Mit dem Parameter ```show_all``` gibt der ```recognizer``` mehrere Alternativen als JSON aus. Integriere dies in deine Erkennung (Tipp: für den weiteren Verlauf ist es ratsam dies als Funktion zu implementieren):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def recognize_google(audio_data: any):\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data, language='de-DE', show_all=True)\n",
    "        print(f\"[Google] Recognition results: {json.dumps(text, sort_keys=True, indent=4)}\")\n",
    "    except sr.RequestError:\n",
    "        print(\"[Google] Fehler: Google Web Speech API ist nicht erreichbar\")\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"[Google] Sorry, Ich verstehe die Äußerung nicht\")\n",
    "recognize_google(audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spracherkennung ohne Cloud und Internetanbindung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerade als Ingenieure haben wir oftmals mit einer fehlenden Internetverbindung in Produktionsumgebungen, begrenzten Hardware-Ressourcen, Datenschutzanforderungen oder was auch immer zu tun, das es verhindert Daten in die Cloud zu schicken. Für solche Fälle möchten wir uns mit https://alphacephei.com/vosk/ einen Spracherkenner anschauen, der einerseits lokal und andererseits auch mit begrenzen Ressourcen zurecht kommt (z.B. Raspberry PI). Installiere die Bibliothek mit ```pip```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install vosk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vosk unterstützt verschiedene Sprachen und bietet Sprachmodelle in unterschiedlicher Größe an: https://alphacephei.com/vosk/models. Starte mit einem kleinen Modell (vosk-model-small-**), lade dieses herunter und nutze die Funktion ```recognizer.recognize_vosk```, um deine Äußerung zu transkribieren. Auch hier ist es hilfreich dies als Funktion zu implementieren...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sr.Microphone() as source:\n",
    "    print(\"Speak something...\")\n",
    "    audio_data = recognizer.listen(source)\n",
    "    print(\"Audio data recorded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_vosk(audio_data: any):\n",
    "    try:\n",
    "        text = recognizer.recognize_vosk(audio_data, language='de')\n",
    "        text_json = json.loads(text)\n",
    "        print(\"[Vosk] You said:\", text_json['text'])\n",
    "    except sr.RequestError:\n",
    "        print(\"[Vosk] Error: Could not access Google Web Speech API;\")\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"[Vosk] Sorry, I do not understand\")\n",
    "recognize_vosk(audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herausforderungen der Spracherkennung\n",
    "\n",
    "Wir haben nun zwei Erkenner angebunden, einerseits ein großes Modell in der Cloud und andererseits ein kleines Modell, welches lokal erkennt. Doch wie gut funktionieren diese? Und wo sind die Unterschiede? Nehme dir etwas Zeit und versuche verschiedene Äußerungen aus, variiere dabei zum Beispiel:\n",
    "- Anwendungsfälle\n",
    "- Namen von Personen, Städten, Restaurants, ...\n",
    "- Sprechgeschwindigkeit\n",
    "- Lautstärke\n",
    "- Deutlich vs. undeutliche Aussprache\n",
    "- Dialekt (kann Google Fränkisch?)\n",
    "- Kommandos vs. natürlichsprachlich\n",
    "- Lange vs. kurze Äußerungen\n",
    "- Englische Begriffe in einem deutschen Satz (oder anders rum)\n",
    "- Umgangssprache\n",
    "\n",
    "Speichere dir hierfür die mit dem Mikrofon aufgenommenen Daten in eine Variable ```audio_data``` und gebe diese Daten an ```recognize_google(audio_data, ..``` und ```recognize_vosk(audio_data, ..```. Hier ist es nun praktisch auf die zuvor erstellen Funktionen zurückzugreifen ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sr.Microphone() as source:\n",
    "    print(\"Speak something...\")\n",
    "    audio_data = recognizer.listen(source)\n",
    "    print(\"Audio data recorded.\")\n",
    "    \n",
    "    recognize_google(audio_data)\n",
    "    recognize_vosk(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
