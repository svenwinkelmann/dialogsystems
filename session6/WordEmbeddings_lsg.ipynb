{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordEmbeddings\n",
    "\n",
    "Ziele der heutigen Übung:\n",
    "- WordEmbeddins anwenden\n",
    "- Semantic Similarity berechnen können"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein guter Artikel für diese Übung ist: https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/  Lese ihn durch und wende ihn für diese Übung an.\n",
    "\n",
    "Installiere als erstes die beiden Bibliotheken ```nltk```und ```gensim```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir möchten in der heutigen Übung ein Modell von WordEmbeddings auf Basis eines eigenen Textes generieren. Der Text sollte eine gewisse Länge haben, ein Buch ist nicht verkehrt. Freie Bücher findet man zum Beispiel auf https://www.gutenberg.org/\n",
    "\n",
    "Lade den Text aus dem entsprechenden File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOK = \"../data/alice_im_wunderland.txt\"\n",
    "\n",
    "text = \"\"\n",
    "with open(BOOK, encoding='utf8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing und explorative Datenanalyse ist nie verkehrt, denn auf Basis eines schlechten, ungepflegten Datensatzes kann kein gutes Modell trainiert werden. Ersetze auf jeden Fall Zeilenumbrüche durch Leerzeichen. Je nach Buch sind eventuell auch weitere Anpassungen notwendig. Du solltest danach einen fortlaufenden Text haben. Es empfiehlt sich auch einige Statistiken über den Text auszugeben, um ein Gefühl für dessen Eigenschaften zu bekommen (z.B. Satz-, Wort-, Characteranzahl).Die NLTK Bibliothek hat dafür  einige praktischen Funktionen, die wir nun nutzen möchten. Recherchiere hierzu in der API, welche Funktionen sinnvoll sind:: https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Exploration:\n",
      "\tNumber of characters:  179155\n",
      "\tNumber of words:  36303\n",
      "\tNumber of sentences:  1196\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sentences = sent_tokenize(text, language='german')\n",
    "words = word_tokenize(text, language='german')\n",
    "\n",
    "print(\"Dataset Exploration:\")\n",
    "print(\"\\tNumber of characters: \", len(text))\n",
    "print(\"\\tNumber of words: \", len(words))\n",
    "print(\"\\tNumber of sentences: \", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nutze nun ```gensim.models.Word2Vec``` um ein CBOW und ein SkipGram WordEmbedding-Modell aus deinem Text zu erstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i in sent_tokenize(text, language='german'):\n",
    "    data.append([x.lower() for x in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "VECTOR_SIZE = 100\n",
    "WINDOW = 3\n",
    "\n",
    "# Create CBOW model\n",
    "model_cbow = Word2Vec(data, min_count=1, vector_size=VECTOR_SIZE, window=WINDOW)\n",
    "model_cbow.name = \"CBOW\"\n",
    "\n",
    "# Create SkipGram model\n",
    "model_skip_gram = Word2Vec(data, min_count=1, vector_size=VECTOR_SIZE, window=WINDOW)\n",
    "model_skip_gram.name = \"SkipGram\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schau dir bei beiden Modellen einige WordEmbeddings an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding for:  alice\n",
      "\tCBOW:  [-1.508644   -1.4417052   2.2125654  -3.6542668   1.1752645  -3.4046993\n",
      " -0.8015332   2.4277296  -1.2990925  -3.6446226  -3.546769   -0.84109956\n",
      " -1.2387382  -1.1329306   2.27871    -2.4218864  -2.709769   -1.0640365\n",
      "  1.8315328  -1.4546776   1.62603    -1.2125131   2.6834517  -3.0582795\n",
      "  0.43251997  2.1715367  -2.7675838  -0.11855579  0.62176126 -0.32994175\n",
      "  0.31455478  1.8492532   2.6200619  -3.37198    -1.722747    0.19361149\n",
      " -4.1181393  -0.25668055  0.92288685 -6.2770896  -1.748529    0.11668386\n",
      "  0.01992221  1.6744705   1.2690009  -4.800357   -3.0380406  -3.1093757\n",
      "  2.0381167  -0.315546    2.6164594  -3.7813854   1.8640152   0.85524446\n",
      " -0.3313397  -2.0673804  -1.5127361   1.5595939  -0.05787027  1.135287\n",
      "  0.01411045 -0.49057788 -1.5209627  -3.529223   -0.98461956  2.3181045\n",
      "  3.963277    3.309091   -2.337658    4.754774   -3.4429991   2.1712663\n",
      " -1.3652229   2.7782166   0.6447172   1.8988823   0.14896776 -3.4390404\n",
      " -1.7776016   0.9475306  -0.92150056 -4.4017205   1.7216356  -1.6567271\n",
      "  0.85009426 -3.6716528  -1.6599823   3.4506342  -3.099849   -1.5511976\n",
      "  1.8512596  -0.40558347  2.1030881   1.3782312  -2.7908158   3.564753\n",
      "  3.3333871   1.7656933   0.0401078   1.7869172 ]\n",
      "\tSkipGram:  [-0.12600581 -0.3811096   2.228676   -2.6077137   0.6747294  -3.1188416\n",
      " -1.5797781   2.243394   -1.0931277  -0.0055222  -4.9004517   2.0593777\n",
      " -1.1330198  -1.0012131   1.7492415  -2.0701294  -4.479706    1.7148924\n",
      " -1.1201236   2.0330474   0.42334405 -0.39727336  4.2418027  -0.55673116\n",
      "  1.2108217   1.6361278  -1.6027536  -0.3596341   2.417559    0.14260718\n",
      " -2.6833534   2.0844183   1.2397786  -3.4693131  -3.0478053   1.2270021\n",
      " -3.756639    0.632782    1.8579723  -3.927918   -2.1525497  -0.31665683\n",
      " -2.3024445   1.9646844  -1.5140738  -4.396948   -2.0416322  -0.81606793\n",
      "  1.3057094   0.73411036  0.83470607 -5.227472   -0.34742326  2.0623858\n",
      "  1.2806602  -1.4990939   0.08421794  0.7662864  -2.1703148   1.8589593\n",
      "  0.5490135  -0.14378531 -0.7617157  -3.592754   -0.6663214   2.2780497\n",
      "  4.3149333   3.9098864  -3.6595354   4.658843   -2.0057116   0.7946421\n",
      " -0.7586135   2.0244293   2.88488     1.4956236  -1.5028571  -5.1610637\n",
      " -0.8118939  -0.14044692 -3.5057533  -3.7397091  -0.81970745  1.0826498\n",
      " -2.2759666  -2.6528895  -0.6727017   3.8211217  -1.3000628  -1.2123055\n",
      "  1.7849052  -0.55383027  3.711707    1.3486544   0.6286757   5.0765967\n",
      "  3.1852784   1.1744589   0.26274127  2.1159608 ] \n",
      "\n",
      "Word Embedding for:  wunderland\n",
      "\tCBOW:  [-2.290767   -0.3263467   2.7410839   5.665447   -0.11871749 -3.384482\n",
      "  3.1667588   2.4501061  -1.3727877  -1.5563165  -2.2680066  -1.5670049\n",
      " -1.2500232   3.0616357   0.52989846 -1.023786    0.23654865  0.43694258\n",
      "  0.46084875 -2.9057364  -0.15460329 -0.6140717   1.3206195   2.5798388\n",
      " -2.592773    3.7564607   0.9865635   3.5407486   0.73051625  0.7140942\n",
      "  5.3792176  -2.2433443   4.8067427  -4.3393035   3.7959414   2.093685\n",
      "  2.686334   -3.0767462  -1.7599208  -0.23184282 -5.0698943  -5.1402006\n",
      " -1.9777405   1.4054711   2.7499123   0.9576451   2.6153579  -3.1851363\n",
      " -2.401348    2.780943   -0.2852888   0.26465088  0.88796175  0.09960072\n",
      " -0.40195253  0.4961564  -1.8295244   4.3006487   0.01037657  1.4664308\n",
      " -0.815124   -2.2349832  -1.8362391  -1.9397501   1.2359556  -0.80475456\n",
      "  2.1507192  -3.230414   -1.7665442  -2.7229154  -1.2209388   0.2921603\n",
      " -0.04079086  4.4574623   1.2346905   3.0298045   2.4493003  -0.09002621\n",
      "  3.4779491   0.54163563 -2.7107933   0.20117037  1.3417194   3.2252588\n",
      "  2.649637   -2.5965428   1.2162071  -0.9488191  -0.19828865  3.3714354\n",
      " -1.3343749  -1.5855882  -0.07773454  0.9760249   3.2955885   2.762165\n",
      "  0.9573445   0.7459493  -0.40045747  1.884242  ]\n",
      "\tSkipGram:  [-3.6467988  -1.6581855   2.115641    5.581661    0.6368695  -1.7061841\n",
      "  2.0979822   1.012514   -1.1949577  -1.6522013  -0.62058014 -2.2517028\n",
      " -0.8214428   2.7557092  -0.08306822  0.85762763  0.9526436   0.7058166\n",
      " -1.654029   -3.0769682   0.34948164 -0.27770448  1.6990235   2.8804734\n",
      " -1.6024328   1.2402637   0.36245853  2.9400682   0.9082632   0.704076\n",
      "  5.423387   -3.00634     2.2049174  -2.447878    2.5619538  -0.08806048\n",
      "  3.246175   -2.3237894  -2.3922079   0.82989925 -4.6368465  -3.1768105\n",
      "  0.10931341  3.1015854   2.9004295   3.2028072   1.6094202  -3.294314\n",
      " -3.463825    2.9218132  -0.56122553  0.25927943 -0.0261487  -0.58476436\n",
      "  1.3761463   2.1119564  -0.03459134  5.329041   -1.3195426   1.5771232\n",
      " -0.56182057 -3.4869206  -1.6342833  -3.15215    -1.4169817   0.5994825\n",
      "  1.846444   -0.44360417 -3.4967341   1.337164    1.1096455   0.15135288\n",
      "  0.02376919  3.441926    4.7398896   4.6421194   1.6080316   0.29093528\n",
      "  5.0040007  -0.31292057 -2.5953906  -0.503932    1.2047997   1.8252515\n",
      "  2.450728   -2.8208714  -0.5476334  -0.35760787 -0.36661398  3.9092903\n",
      " -0.3184608   0.41572243  0.14937663  0.4099731   2.7866433   3.8590257\n",
      "  0.96952844  1.6335014  -2.047031    4.2213354 ] \n",
      "\n",
      "Word Embedding for:  kaninchen\n",
      "\tCBOW:  [ 3.9622178  -3.685789    8.234939    2.798999   -3.1479847   3.474437\n",
      " -2.68215    -3.919816   -7.776383   -1.4945065   0.5517355  -1.1469622\n",
      "  1.238139   -0.37635362  0.12002134  0.03782983 -2.2470803  -5.1387105\n",
      "  1.583377    0.2970903  -0.5861818   0.7068608   3.1147983   0.16459575\n",
      " -2.0566108   0.63226664  4.0759544  -0.31556487 -0.83519816 -3.3894336\n",
      "  1.452215   -2.7545452  -1.293949   -2.7472484  -3.3757365  -2.2576852\n",
      " -1.6064866  -1.3231888   1.4595821  -4.4023995   1.3910875  -2.5161364\n",
      " -4.764982   -0.830393   -5.8316483   0.2396123   5.822612   -4.1337895\n",
      " -7.0213284  -5.794715    0.5528787  -0.78250444  0.3549701   0.05956505\n",
      " -3.345516   -0.9895607  -1.4657743   6.7020135   1.4231514  -4.7746115\n",
      "  2.663564    0.39052105 -0.23375985 -2.696208    2.2848012  -0.29590085\n",
      " -0.03681096 -7.888922    0.9960743   0.8544228  -2.3629737  -4.6878586\n",
      " -3.761154   -1.878636   -0.03551331  0.043088    1.6883236   0.6772555\n",
      "  0.91254354 -5.126338   -1.0086519  -0.96960616 -0.93747467  1.245067\n",
      "  2.9004204  -2.1040506  -0.4493468   5.077009   -1.0023942  -0.47503302\n",
      "  0.5155847   0.2818153   8.360928    2.6406777   0.0095687   4.2300496\n",
      "  0.98636866  2.7901952   5.9770303  -5.103568  ]\n",
      "\tSkipGram:  [-0.5881688  -4.967874    8.306512    2.3341227  -2.3398123   1.7351296\n",
      " -5.149957   -4.2974477  -6.587462   -2.1551259  -0.16443647 -3.5926926\n",
      "  2.6460922  -0.2958111   0.2625119  -0.8953996  -2.2123685  -4.931634\n",
      "  1.6143814   0.67047703  2.527661    2.5470276   6.7770443   1.9333687\n",
      " -4.3522205  -0.0525322   4.839433   -0.835905    0.9214846  -0.97400826\n",
      "  2.6383708  -3.5935116  -4.7806125   0.8679812  -3.3288076  -4.6321325\n",
      " -6.5484185  -0.3016493  -1.4332639  -4.901129    1.7188487   0.98244345\n",
      " -0.26608688  2.0399625  -1.8396293   2.3468995   3.3631468  -2.5393493\n",
      " -4.440369   -4.0418024   1.9675846  -1.1212302   0.54125786 -1.5353029\n",
      " -2.863661    0.24361566 -0.64492947  4.601025    1.7847108  -4.737194\n",
      "  3.220768   -0.65413344  0.8972032  -4.30159    -0.5735018   1.2923217\n",
      " -1.221224   -5.456218    0.33442178  1.1539488  -1.2710285  -5.808973\n",
      " -1.4961605  -2.391743    2.9255054   2.236876    0.38224286  0.5987624\n",
      "  0.10743143 -5.1982007   1.02623     0.20758009 -0.04359986  0.17545642\n",
      "  1.7213535  -6.3823547  -1.2710819   4.4953833  -1.5635636   1.0044919\n",
      " -1.9037215   0.738473    6.808314    2.9253752  -2.9239016   2.4718416\n",
      "  2.115622    5.625292    3.4197583  -4.9582353 ] \n",
      "\n",
      "Word Embedding for:  haus\n",
      "\tCBOW:  [-3.7583485   1.4669849  -2.5991468   2.806201   -0.74915034  2.1504533\n",
      " -1.9531863  -3.1448407  -0.16425171 -4.3943095  -3.5357144  -1.8590645\n",
      "  1.3222148  -2.8181677  -3.4950266  -0.29153508  1.9741422  -1.4909426\n",
      "  3.7075114  -0.45655656 -0.47045267 -0.00706986 -2.9705505   0.65289086\n",
      "  0.9201854  -4.7781534   5.6408653  -1.1599033  -2.3186479  -3.6076443\n",
      " -0.08084633 -1.7093443  -1.4962475  -3.608687    1.6360667  -2.9057026\n",
      "  2.5151575   0.6317514  -0.22455329 -2.775116    0.8413134  -1.3599031\n",
      "  0.7969339  -1.149404   -0.33973303  0.46517712  2.3097653   3.367265\n",
      " -2.1316087  -0.10554824 -1.1974391   1.8922455  -1.1596661  -1.4064127\n",
      "  1.3697942   1.0279746  -0.65511096 -2.483036   -1.1445748  -0.7237969\n",
      "  1.5703121   2.0401056   2.7082448  -4.0746293   1.873055   -2.9591317\n",
      "  0.55478597 -3.859334    3.1574266  -4.224762    1.215321    4.369311\n",
      " -0.279196    2.1432552   0.3416451  -1.3801706   1.7944281   4.6956964\n",
      " -0.43790486  0.2093331   1.3791807   3.914492    0.15029618 -3.172846\n",
      "  1.2673936  -1.3171942  -2.6463683   1.537733    2.8230872   1.2319435\n",
      " -0.8243256  -0.9008955   1.9716046   0.2608982   2.4985747   3.829571\n",
      "  3.1282082   2.8295956   0.7725375   6.96721   ]\n",
      "\tSkipGram:  [-4.5936828e+00  5.1498687e-01  7.0589088e-02  2.2355769e+00\n",
      " -8.1456339e-01  1.9204465e+00 -1.9176978e+00 -3.2680626e+00\n",
      " -3.1271477e+00 -2.8703980e+00 -1.9387454e+00 -2.3685880e+00\n",
      "  3.0850260e+00 -2.5716138e+00 -2.2222812e+00 -1.6146490e-01\n",
      "  1.9994187e+00 -1.7084836e+00  4.0095739e+00  2.1243544e+00\n",
      " -2.8194053e+00  2.5886147e+00 -4.4235315e+00  1.5259596e+00\n",
      "  1.2822704e+00 -5.7105517e+00  2.6164246e+00 -2.2928398e+00\n",
      " -1.0821899e+00 -1.1138675e+00 -1.3526058e+00 -1.7262975e-02\n",
      " -2.5133371e+00 -1.0025750e+00  1.6334840e+00 -6.0006561e+00\n",
      "  2.0051010e+00  1.9434108e+00  2.5249910e+00 -2.0640240e+00\n",
      "  1.1528711e+00 -1.8017485e+00  3.0895507e+00 -6.1861646e-01\n",
      "  1.1152585e+00  2.6172979e+00  1.6359062e-01  1.7709799e+00\n",
      " -1.8301536e+00  7.1718228e-01  1.0597256e+00 -6.7960358e-01\n",
      "  4.3786514e-01  6.5789765e-01  1.0610049e+00  2.3198261e+00\n",
      " -1.4975058e+00 -3.0654140e+00  5.1732093e-01 -1.4148234e+00\n",
      " -1.8638760e-01  2.1156676e-01  3.4687126e+00 -1.1822524e+00\n",
      " -4.4350913e-01 -3.7694985e-01  5.1252372e-03 -4.8938042e-01\n",
      "  1.5671989e+00 -4.6882883e-01  2.2072880e+00  4.5133405e+00\n",
      "  1.4880975e+00  3.4633074e+00  2.3238344e+00 -3.2331225e-01\n",
      " -1.7790106e-01  5.1963229e+00  1.3405299e+00 -5.3593922e-01\n",
      "  1.7159376e+00  4.3075976e+00  2.3494549e+00 -1.3070450e+00\n",
      " -3.0875176e-01 -3.7363534e+00 -4.4274549e+00  3.8415849e+00\n",
      " -8.8140804e-01  1.2297999e+00  2.2717500e-01 -1.4291321e+00\n",
      "  5.4035711e-01  1.3368474e+00  2.0833092e+00  5.7880831e-01\n",
      "  1.7239895e+00  4.0072632e+00 -1.2431176e+00  6.2213612e+00] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in ['alice', 'wunderland', 'kaninchen', 'haus']:\n",
    "    print(\"Word Embedding for: \", w)\n",
    "    print(f\"\\t{model_cbow.name}: \", model_cbow.wv[w])\n",
    "    print(f\"\\t{model_skip_gram.name}: \", model_skip_gram.wv[w], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Vorlesung haben wir die Ähnlichkeit von Wörtern mit der Kosinus-Ähnlichkeit zwischen zwei WordEmbedding Vektoren berechnet. Nutze die Formel um zwischen zwei Vektoren die Ähnlichkeit zu bestimmen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simliarity between alice and wunderland (CBOW): 0.13673104345798492\n",
      "Simliarity between alice and wunderland (SkipGram): 0.09977753460407257\n",
      "Simliarity between alice and kaninchen (CBOW): 0.10536202788352966\n",
      "Simliarity between alice and kaninchen (SkipGram): 0.11443577706813812\n",
      "Simliarity between alice and haus (CBOW): -0.1560482233762741\n",
      "Simliarity between alice and haus (SkipGram): -0.14914971590042114\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "sim_tuple = (('alice', 'wunderland'), ('alice', 'kaninchen'), ('alice', 'haus'))\n",
    "\n",
    "for tuple in sim_tuple:\n",
    "    for model in [model_cbow, model_skip_gram]:\n",
    "        vec1 = model.wv[tuple[0]]\n",
    "        vec2 = model.wv[tuple[1]]\n",
    "        simliarity = dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "        print(f\"Simliarity between {tuple[0]} and {tuple[1]} ({model.name}): {simliarity}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nutze nun die ```similarity``` Funktion, ist das Ergebnis gleich deiner eigenen Berechnung? Woran könnte der Unterschied liegen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simliarity between alice and wunderland (CBOW): 0.13673105835914612\n",
      "Simliarity between alice and wunderland (SkipGram): 0.09977751970291138\n",
      "Simliarity between alice and kaninchen (CBOW): 0.10536204278469086\n",
      "Simliarity between alice and kaninchen (SkipGram): 0.11443576961755753\n",
      "Simliarity between alice and haus (CBOW): -0.15604820847511292\n",
      "Simliarity between alice and haus (SkipGram): -0.14914970099925995\n"
     ]
    }
   ],
   "source": [
    "for tuple in sim_tuple:\n",
    "    for model in [model_cbow, model_skip_gram]:\n",
    "        simliarity = model.wv.similarity(tuple[0], tuple[1])\n",
    "        print(f\"Simliarity between {tuple[0]} and {tuple[1]} ({model.name}): {simliarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interessant ist die Funktion ```most_similar``` lese dir die Doku durch und probiere einige Wörter deines Buches aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zusammen', 0.3532821834087372),\n",
       " ('hastig', 0.3434976637363434),\n",
       " ('niederbrennen', 0.34009143710136414),\n",
       " ('glücklicherweise', 0.3340270519256592),\n",
       " ('einmaleins', 0.3179190158843994),\n",
       " ('mittel', 0.31787824630737305),\n",
       " ('etwas', 0.3178246021270752),\n",
       " ('wolltest.', 0.31777095794677734),\n",
       " ('sie', 0.3145189881324768),\n",
       " ('der', 0.31451767683029175)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['alice', 'wunderland', 'kaninchen'], negative=['mann'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
